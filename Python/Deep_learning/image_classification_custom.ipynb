{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXtkSoLWoFwO",
        "outputId": "89ff2428-f090-4ab9-af8f-897c912fe6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 8 directories and 300 images in 'data/images'.\n",
            "Epoch: 1 | Train Accuracy: 0.2930 | Test Accuracy: 0.2604\n",
            "Epoch: 2 | Train Accuracy: 0.3086 | Test Accuracy: 0.5521\n",
            "Epoch: 3 | Train Accuracy: 0.3008 | Test Accuracy: 0.3703\n",
            "Epoch: 4 | Train Accuracy: 0.4219 | Test Accuracy: 0.2604\n",
            "Epoch: 5 | Train Accuracy: 0.4297 | Test Accuracy: 0.2604\n",
            "Epoch: 6 | Train Accuracy: 0.4375 | Test Accuracy: 0.2604\n",
            "Epoch: 7 | Train Accuracy: 0.4453 | Test Accuracy: 0.3021\n",
            "Epoch: 8 | Train Accuracy: 0.4336 | Test Accuracy: 0.2604\n",
            "Epoch: 9 | Train Accuracy: 0.5273 | Test Accuracy: 0.3125\n",
            "Epoch: 10 | Train Accuracy: 0.4219 | Test Accuracy: 0.4138\n",
            "Total training time: 23.319 seconds\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import random\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "\n",
        "# Setup the device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set a fixed random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"images\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it\n",
        "if not image_path.is_dir():\n",
        "    print(f\"Directory doesn't exist, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download images\n",
        "    with open(data_path / \"images.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "        print(\"Downloading images\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip images\n",
        "    with zipfile.ZipFile(data_path / \"images.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping images...\")\n",
        "        zip_ref.extractall(image_path)\n",
        "\n",
        "# Function to walk through directories and count files\n",
        "def walk_through_dir(dir_path):\n",
        "    num_dirs = sum([len(dirs) for _, dirs, _ in os.walk(dir_path)])\n",
        "    num_files = sum([len(files) for _, _, files in os.walk(dir_path)])\n",
        "    print(f\"There are {num_dirs} directories and {num_files} images in '{dir_path}'.\")\n",
        "\n",
        "walk_through_dir(image_path)\n",
        "\n",
        "# Setup training and testing paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "# Transform images to datasets\n",
        "data_transform = transforms.Compose([\n",
        "   transforms.Resize(size=(64, 64)),  # resize the image to 64x64\n",
        "   transforms.RandomHorizontalFlip(p=0.5),  # flip the images randomly on the horizontal\n",
        "   transforms.ToTensor()  # Turn the image to a tensor\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(root=train_dir, transform=data_transform)\n",
        "test_data = datasets.ImageFolder(root=test_dir, transform=data_transform)\n",
        "\n",
        "# Turn train and test Datasets into DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "# Model definition\n",
        "class TinyVGG(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=hidden_units*16*16, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Create train_step(), test_step(), train()\n",
        "def train_step(model: nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Function to perform one training step\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model\n",
        "        dataloader (torch.utils.data.DataLoader): Training data loader\n",
        "        loss_fn (nn.Module): Loss function\n",
        "        optimizer (torch.optim.Optimizer): Optimizer\n",
        "\n",
        "    Returns:\n",
        "        float: Training accuracy\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    train_acc = 0\n",
        "\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        y_pred_class = y_pred.argmax(dim=1)\n",
        "        train_acc += (y_pred_class == y).float().mean().item()\n",
        "\n",
        "    train_acc /= len(dataloader)\n",
        "    return train_acc\n",
        "\n",
        "def test_step(model: nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: nn.Module):\n",
        "    \"\"\"\n",
        "    Function to perform one testing step\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model\n",
        "        dataloader (torch.utils.data.DataLoader): Testing data loader\n",
        "        loss_fn (nn.Module): Loss function\n",
        "\n",
        "    Returns:\n",
        "        float: Testing accuracy\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X,y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            test_pred_logits = model(X)\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += (test_pred_labels == y).float().mean().item()\n",
        "\n",
        "    test_acc /= len(dataloader)\n",
        "    return test_acc\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: nn.Module = nn.CrossEntropyLoss(),\n",
        "          epochs: int = 5):\n",
        "    \"\"\"\n",
        "    Function to train the model\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model\n",
        "        train_dataloader (torch.utils.data.DataLoader): Training data loader\n",
        "        test_dataloader (torch.utils.data.DataLoader): Testing data loader\n",
        "        optimizer (torch.optim.Optimizer): Optimizer\n",
        "        loss_fn (nn.Module, optional): Loss function. Defaults to nn.CrossEntropyLoss().\n",
        "        epochs (int, optional): Number of epochs for training. Defaults to 5.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing training and testing accuracies\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        \"train_acc\": [],\n",
        "        \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_acc = train_step(model=model,\n",
        "                               dataloader=train_dataloader, loss_fn=loss_fn,\n",
        "                               optimizer=optimizer)\n",
        "        test_acc = test_step(model=model,\n",
        "                             dataloader=test_dataloader,\n",
        "                             loss_fn=loss_fn)\n",
        "\n",
        "        # Print out training and testing accuracies for each epoch\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"Train Accuracy: {train_acc:.4f} | \"\n",
        "            f\"Test Accuracy: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Define data augmentation and transformation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create augmented training dataset and simple testing dataset\n",
        "train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "test_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "# Create data loaders for augmented training data and simple testing data\n",
        "train_dataloader_augmented = DataLoader(train_data_augmented,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        num_workers=NUM_WORKERS,\n",
        "                                        shuffle=True)\n",
        "test_dataloader_simple = DataLoader(test_data_simple,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    num_workers=NUM_WORKERS,\n",
        "                                    shuffle=False)\n",
        "\n",
        "# Initialize the model\n",
        "model_1 = TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(train_data_augmented.classes)).to(device)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)  # Adjust learning rate\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.01)\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "model_1_results = train(model=model_1,\n",
        "                        train_dataloader=train_dataloader_augmented,\n",
        "                        test_dataloader=test_dataloader_simple,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS)\n",
        "\n",
        "# End the timer and print out the training time\n",
        "end_time = time.time()\n",
        "print(f\"Total training time: {end_time - start_time:.3f} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-AoixRXYoNI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}